\section{Probabilistic Planning}
\subsection{Markov Decision Processes}
An MDP is specified by a quintuple: $(X, A, r, P(x'|x, a), \gamma)$, where $X$ are states, $A$ are actions, $r(x,a)$ is a reward function and transition probabilities:\\
    $P(x'|x,a)=\text{Prob(Next state}=x'|\text{Action } a)$\\ 
\textbf{Goal:} Maximize $J(\pi) = \E[\sum_{k\geq0} \gamma^k r(X_k, \pi(X_k))] = \sum_{x}P(X_0=x)V^\pi(x)$.\\
\textbf{Value function:} $V^\pi(x) = J(\pi|X_0=x) = r(x,\pi(x)) + \gamma\sum_{x'} P(x'|x,\pi(x))V^\pi(x')$.\\
\emph{Closed form:} $V^{\pi} = (\mathbf{I}-\gamma P^\pi)^{-1}r^\pi$. \emph{Fixed-point it.:} $\mathcal{B}V = r^\pi+\gamma P^\pi V$ contraction, so $||V_t^\pi-V^\pi||_\infty \leq \gamma^t||V_0^\pi-V^\pi||_\infty$.\\
\textbf{Greedy policy wrt. V:}\\
$\pi(x)=\argmax_{a\in A} [r(x,a)+\gamma \sum_{x'}P(x'|x,a)V(x')]$\\
\textbf{Theorem (Bellman)}: A policy is optimal iff it is greedy w.r.t. its induced value function!\\
\mbox{$\pi^*(x)={\argmax_{a}} [r(x,a)+\gamma \sum_{x'}P(x'|x,a)V^{\pi*}(x')]$}\\
$V^*(x)=\max_a[r(x,a)+\gamma \sum_{x'}P(x'|x,a)V^*(x')]$\\


\textbf{Policy iteration (Cost $O((|X|^3+|X|^2|A|)K$):}\\
Start with an arbitrary (e.g. random) policy $\pi$.
Until converged, do:\\
- Compute value function $V^\pi (x)$\\
- Compute greedy policy $\pi_G$ w.r.t. $V^\pi$\\
- Set $\pi \leftarrow \pi_G$\\
Guaranteed to \emph{monotonically improve} and to converge to an \emph{optimal} policy $\pi^*$ in \emph{polynomial} ($O(n^2m/(1-\gamma))$) iterations!

\textbf{Value iteration (Cost $O(|X|^2|A|)K$):}\\
Initialize $V_0(x)=max_a r(x,a)$\\
For $t=1$ to $\infty$:\\
- For each $(x,a)$, let: \\
$Q_t(x,a)=r(x,a)+\gamma\sum_{x'}P(x'|x,a)V_{t-1}(x')$\\
- For each $x$, let $V_t(x)=\underset{a}{max}Q_t(x,a)$\\
- Break if $||V_{t+1}-V_{t}||_{\infty}{\leq\color{gray}\gamma||V_t-V_{t-1}||_{\infty}}\leq\epsilon$\\
Then choose greedy policy w.r.t $V_t$.\\
CV proof: Bellman update $(B^*V)(x) = \max_a Q(x,a)$ is a contraction: $||B^*V-B^*V'||_\infty \leq \gamma||V-V'||_\infty$\\
Guaranteed to converge to $\epsilon$-optimal policy in \emph{polynomial} iterations!

\textbf{POMDP = Partially observable MDP):}\\
\emph{Graphical model =} Kalman filter + actions.\\
States = probability dists. over states X =\\
\mbox{$B=\Delta(\{1,...,n\})=\{ b:{1,...,n} \rightarrow [0,1],\sum_x b(x)=1 \}$}\\
Actions: Same as original MDP.\\
\textbf{Transition model:}\\
Stochastic observation:\\
$P(Y_{t+1}=y|b_t,a)=\sum_{x,x'} P(x'|x,a)P(y|x')$\\
State update (Bayesian filtering!):
$b_{t+1}(x') = P(X_{t+1}=x'|y_{1:t+1},a_{1:t})=$\\$\frac{1}{Z}\sum_xb_t(x)P(y_{t+1}|x)P(X_{t+1}=x'|X_t=x,a_t)$\\
Reward function: $r(b_t, a_t)=\sum_x b_t(x)r(x,a_t)$
\emph{Solve using \textbf{dynamic programming} or \textbf{policy gradients}}.

\subsection{Example of approx. solution to POMDPs: Policy gradients}
- Assume parameterized policy: $\pi(b)=\pi(b;\theta)$\\
- For each parameter $\theta$ the policy induces a Markov chain\\
- Can compute expected reward $J(\theta)$ by sampling.\\
- Find optimal parameters through search (gradient ascent):
$\theta^* = \underset{\theta}{arg max}\quad J(\theta)$


