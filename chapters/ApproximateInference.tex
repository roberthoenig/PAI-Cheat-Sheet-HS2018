
\section{Approximate inference}
\subsection{Sampling based inference (MCMC)}
\textbf{Goal}: Sample from $\frac{1}{Z}Q(X)$ to approximate
$\E_{\theta\sim\frac{1}{Z}Q(.)}[f(\theta)]$. ($f(\theta) = p(y^*|x^*,\theta), Q(\theta)\propto p(\theta|x_{1:n},y_{1:n}))$ for Bayesian Inference)\\
\textbf{Hoeffding's inequality:} If $f$ is bounded in $[0, C]$, then
$P(|E[f(X)]-\frac{1}{N}\sum_{i=1}^N f(x_i)| > \epsilon) \leq 2exp\big( \frac{-2N\epsilon^2}{C^2}\big)$.\\
\textbf{Ergodic theorem:} If $(X_i)$ is an ergodic MC with stationary distribution $\pi$, then
$\lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}f(X_i) = \E_{X\sim\pi}[f(X)]$. 

\subsubsection{Markov Chains}
A (stationary) MC is a sequence of RVs $X_1, ..., X_N$, with prior $P(X_1)$ and transition probabilities $P(X_{t+1}|X_t)$ independent of t.\\
\textbf{Markov assumpt.:} $X_{1:t-1}\perp X_{t+1:T}|X_t$, $\forall t>1$\\
\textbf{Stationarity assumption:}\\
$P(X_{t+1}=x|X_t=x')=P(X_{t}=x|X_{t-1}=x')$, $\forall t>1$\\
$\pi$ is a \textbf{stationary distribution} if $\pi^TP = \pi^T$.\\
If \textbf{ergodic} ($\exists t$ s.t. every state can be reached in exactly t steps), then: MC has a unique and positive stationary distribution $\pi$, s.t. for all $x$:\\
            $\underset{N\rightarrow\infty}{lim}P(X_N=x)=\pi(x)$ for any prior $P(X_1)$.\\
If MC satisfies the \textbf{detailed balance equation (DBE)} ($Q(x)P(x'|x)=Q(x')P(x|x')$), then it has stationarity distribution $\pi(x)=\frac{1}{Z}Q(x)$.

\subsubsection{Metropolis-Hastings}
0. Design some proposal distribution $R(X'|X)$.
1. Assume $X_t = x$. Sample $x' \sim R(X'|X=x)$.\\
2. Compute $\alpha = \min(1, \frac{Q(x')R(x'|x)}{Q(x)R(x|x')})$.\\
3. Set $X_{t+1} = x'$ with prob. $\alpha$, else $X_{t+t} = x$.
{\tiny\CircArrowRight{}}\\
\emph{Works because the detailed balance eq. is satisfied.}
\subsubsection{Gibbs sampling (for random vectors)}
0. Initialize $\rvx$.\\
1. Sample $i\sim\mathcal{U}[1,\ldots,n]$.\emph{ Satisfies DBE}\\
\emph{1. (Practical variant): Increment $i$ (Doesn't satisfy DBE, but still correct stationary distribution.)}\\
2. Sample $x_i$ from $P(X_i|X_{-i}=\rvx_{-i})$.{\tiny\CircArrowRight{}}\\
\emph{Requires efficient comp. of $P(X_i=x_i|X_{-i}=\vx_{-i})$}.

\subsubsection{MALA}
Let $p(\rvx) = \exp(-f(x))$ (f = \textbf{energy function}).\\
Do Metropolis-Hastings with $R(\rvx'|\rvx) = \mathcal{N}(\rvx';\rvx-\tau\nabla f(\rvx), 2\tau\mI)$ (moves quicker towards high-density regions of $p$).

\emph{\textbf{Mixing time} polynomial in len($\rvx$) for \textbf{log-concave} distributions (i.e. convex $f$)}.

\subsubsection{SGLD (= MALA + stochastic gradients \& always accept proposal)}
1. Sample $i_1,\ldots,i_m \sim \mathcal{U}\{1,\ldots,n\}$.\\
2. Update $\rvx_{t+1} \sim \mathcal{N}(\rvx_t + \eta_t(\nabla p(\theta_t)+n\frac{1}{m}\sum_{j=1}^m\nabla p(y_{i_j}|\theta_t,x_{i_j})), 2\eta_t\mI)$

\emph{Converges if $\eta_t \in \Theta(t^{-1/3})$. Add momentum to get HMC}