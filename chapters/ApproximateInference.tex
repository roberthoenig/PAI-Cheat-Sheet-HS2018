
\section{Approximate inference}
\subsection{Variational Inference}
\subsubsection*{Bayesian Logistic Regression:}
\textbf{Data:} $\vx_i \in R^n$, $y_i\in\{-1,1\}$.
\textbf{Prior:} $\rvw\sim\mathcal{N}(0,\sigma^2_p\mathbf{I})$.
\textbf{Likelihood:} $p(\rvy|\rvw) = \prod\sigma(y_i\rvw^T\vx_i)$.\\
\textbf{Posterior:} $p(\rvw|\rvy)$ intractable.
\subsubsection*{Laplace approximation:}
Fit Gaussian around mode of posterior:\\\hphantom\,\,\,\,\,$p(\rvw|\rvy)\approx\mathcal{N}(\rvw;\mu,\Lambda^{-1})$ where $\mu = \argmax_\rvw{p(\rvw|\rvy)}$ and $\Lambda = -\nabla\nabla p(\mu|\rvy)$. (Use Taylor expansion of $\log p(\rvw|\rvy)$.)
\emph{For Bayesian Logistic Regression}: Find $\mu$ with GD on $p(\rvw|\rvy)$. $\Lambda = \sum \vx_i\vx_i^T\sigma(\mu^T \vx_i)(1-\sigma(\mu^T \vx_i))$. Predictive dist.: $p(y^*|\vx^*,\rvy) \approx \int\sigma(y^*\rvw^T\vx^*)\mathcal{N}(\rvw;\mu,\Lambda^{-1}) \dd{\vw}   = \int\sigma(y^*f)\mathcal{N}(f;\mu^T\vx^*, {x^*}^T\Lambda^{-1}x^*) \dd{f}$.
\subsubsection*{Variational Inference}
Given variational family Q, find $\argmin_{q\in Q}\mathrm{KL}(q||p)$ (reverse KL). We have\\
(1) $\mathrm{KL}(q||p) = \int q(\theta)\log\frac{q(\theta)}{p(\theta)}\dd{\theta}$\\ 
(2) $\mathrm{KL}(\mathcal{N}(\mu_0,\Sigma_0),\mathcal{N}(\mu_1,\Sigma_1)) = \frac{1}{2}(\tr(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^T\Sigma_1^{-1}(\mu_1-\mu_0))-d+\ln\frac{|\Sigma_1|}{|\Sigma_0|}$.\\
(3) $\argmin_{q}\mathrm{KL}(q||p(\cdot|\rvy))) = \argmax_q \mathrm{\color{black}\mathbf{ELBO}}$ where $\mathrm{\color{black}\mathbf{ELBO}} = \E_{\theta\sim q}[\log p(\theta, \rvy)] + H(q) = \E_{\theta\sim q}[\log p(y|\theta)] - \mathrm{KL}(q||p(\cdot))$.

\subsection{Sampling based inference (MCMC)}
\textbf{Goal}: Sample from $\frac{1}{Z}Q(X)$ to approximate
$\E_{\theta\sim\frac{1}{Z}Q(.)}[f(\theta)]$. ($f(\theta) = p(y^*|x^*,\theta), Q(\theta)\propto p(\theta|x_{1:n},y_{1:n}))$ for Bayesian Inference)\\
\textbf{Hoeffding's inequality:} If $f$ is bounded in $[0, C]$, then
$P(|E[f(X)]-\frac{1}{N}\sum_{i=1}^N f(x_i)| > \epsilon) \leq 2exp\big( \frac{-2N\epsilon^2}{C^2}\big)$.\\
\textbf{Ergodic theorem:} If $(X_i)$ is an ergodic MC with stationary distribution $\pi$, then
$\lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}f(X_i) = \E_{X\sim\pi}[f(X)]$. 

\subsubsection{Markov Chains}
A (stationary) MC is a sequence of RVs $X_1, ..., X_N$, with prior $P(X_1)$ and transition probabilities $P(X_{t+1}|X_t)$ independent of t.\\
\textbf{Markov assumpt.:} $X_{1:t-1}\perp X_{t+1:T}|X_t$, $\forall t>1$\\
\textbf{Stationarity assumption:}\\
$P(X_{t+1}=x|X_t=x')=P(X_{t}=x|X_{t-1}=x')$, $\forall t>1$\\
$\pi$ is a \textbf{stationary distribution} if $\pi^TP = \pi^T$.\\
If \textbf{ergodic} ($\exists t$ s.t. every state can be reached in exactly t steps), then: MC has a unique and positive stationary distribution $\pi$, s.t. for all $x$:\\
            $\underset{N\rightarrow\infty}{lim}P(X_N=x)=\pi(x)$ for any prior $P(X_1)$.\\
If MC satisfies the \textbf{detailed balance equation (DBE)} ($Q(x)P(x'|x)=Q(x')P(x|x')$), then it has stationarity distribution $\pi(x)=\frac{1}{Z}Q(x)$.

\subsubsection{Metropolis-Hastings}
0. Design some proposal distribution $R(X'|X)$.
1. Assume $X_t = x$. Sample $x' \sim R(X'|X=x)$.\\
2. Compute $\alpha = \min(1, \frac{Q(x')R(x'|x)}{Q(x)R(x|x')})$.\\
3. Set $X_{t+1} = x'$ with prob. $\alpha$, else $X_{t+t} = x$.
{\tiny\CircArrowRight{}}\\
\emph{Works because the detailed balance eq. is satisfied.}
\subsubsection{Gibbs sampling (for random vectors)}
0. Initialize $\rvx$.\\
1. Sample $i\sim\mathcal{U}[1,\ldots,n]$.\emph{ Satisfies DBE}\\
\emph{1. (Practical variant): Increment $i$ (Doesn't satisfy DBE, but still correct stationary distribution.)}\\
2. Sample $x_i$ from $P(X_i|X_{-i}=\rvx_{-i})$.{\tiny\CircArrowRight{}}\\
\emph{Requires efficient comp. of $P(X_i=x_i|X_{-i}=\vx_{-i})$}.

\subsubsection{MALA}
Let $p(\rvx) = \exp(-f(x))$ (f = \textbf{energy function}).\\
Do Metropolis-Hastings with $R(\rvx'|\rvx) = \mathcal{N}(\rvx';\rvx-\tau\nabla f(\rvx), 2\tau\mI)$ (moves quicker towards high-density regions of $p$).

\emph{\textbf{Mixing time} polynomial in len($\rvx$) for \textbf{log-concave} distributions (i.e. convex $f$)}.

\subsubsection{SGLD (= MALA + stochastic gradients \& always accept proposal)}
1. Sample $i_1,\ldots,i_m \sim \mathcal{U}\{1,\ldots,n\}$.\\
2. Update $\rvx_{t+1} \sim \mathcal{N}(\rvx_t + \eta_t(\nabla p(\theta_t)+n\frac{1}{m}\sum_{j=1}^m\nabla p(y_{i_j}|\theta_t,x_{i_j})), 2\eta_t\mI)$

\emph{Converges if $\eta_t \in \Theta(t^{-1/3})$. Add momentum to get HMC}