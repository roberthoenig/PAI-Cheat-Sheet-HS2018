\section{Reinforcement Learning}
\emph{Unknown $r(x,a)$, $P(x'|x,a)$.}
\subsection{Model-based, $|A|$ small, $|X|$ small}
\mbox{1. Empirically estimate $P(x'|x,a)$ and $r(x,a)$.}\\
2. Update $\pi$. \quad 3. Explore \& exploit:\\
3.1 ($\color{black}\mathbf{\epsilon}$\textbf{-greedy}): $a_t=\mathrm{Cat}_{[1-\epsilon_t, \epsilon_t]}[\pi^*(x_t),\mathrm{rand}]$. CV a.s. if $\sum \epsilon_t = \infty$, $\sum \epsilon^2_t < \infty$.\\
3.2 ($\color{black}\mathbf{R_{\max}}$): Set $r(x,a)=R_{\max}$, $P(x^*|x,a)=1$, $r(x^*,a)=R_{\max}$. Update with empirical estimates after collecting $n_{x,a} \geq \frac{R_{\max}^2}{2\epsilon^2}\log\frac{2}{\delta}$ samples. \emph{Every $T$ steps, w.h.p. $R_{\max}$ yields near-opt. reward or visits $\geq 1$ unknown state-action pair.\\}
\emph{With $p=1-\delta$ $R_{\max}$ reaches $\epsilon$-opt. policy in $O(\mathrm{poly}(|X|,|A|,T,\frac{1}{\epsilon},\log\frac{1}{\delta}, R_{\max})$)}
\subsection{Model-free, $|A|$ small, $|X|$ small}
\textbf{TD-Learning:} \emph{Goal:} Find $V^\pi$ given $\pi$.\\
\emph{Algorithm:} 1. Follow $\pi$ to get $(x,a,r,x')_t$. 2.Update $\hat{V}^\pi(x) \leftarrow (1-\alpha_t)\hat{V}^\pi(x_t) + \alpha_t (r + \gamma\hat{V}^\pi(x_t'))$.
\emph{If $\sum\alpha_t=\infty$, $\sum\alpha_t^2<\infty$ and every (x,a) is visited $\color{black}\infty$-ly often, then a.s. $\hat{V}^\pi \to V^\pi$.}\\
\textbf{{\color{gray} Optimistic} Q-Learning:} {\color{gray} \emph{\textbf{Init:}} $\color{gray}\hat{Q}^*(x,a) = \frac{R_{\max}}{1-\gamma}\prod_{t=1}^{T_\mathrm{init}}(1-\alpha_t)^{-1}$}.
\emph{\color{gray}\textbf{Explore:}} $\color{gray}a_t \in \argmax_a \hat{Q}^*(x,a)$. \emph{\textbf{Update:}} $\hat{Q}^*(x_t,a_t) \leftarrow (1-\alpha_t)\hat{Q}^*(x_t,a_t) + \alpha_t(r_t + \gamma(\max_a\hat{Q}^*(x'_t,a)$.\\
\emph{If $\sum\alpha_t=\infty$, $\sum\alpha_t^2<\infty$ and every (x,a) is visited $\color{black}\infty$-ly often, then a.s. $\hat{Q}^* \to Q^*$.}\\
\emph{\color{gray} With $\color{gray}p=1-\delta$ Optimistic Q-Learning reaches $\epsilon$-opt. policy in $O(\mathrm{poly}(|X|,|A|,T,\frac{1}{\epsilon},\log\frac{1}{\delta})$)}
\subsection{Model-free, $|A|$ small, $|X|$ large}
\textbf{Deep Q-Learning:} Define $Q(x,a;\theta) \approx Q^*(x,a)$.
\textbf{\emph{Training:}} Minimize $ l_2(x,a,r,x',\theta) = \frac{1}{2}\delta^2$ with $\delta = Q(x,a;\theta) - (r+\gamma\max_{a'}Q(x',a';\theta_\mathrm{old}))$.\\
\emph{Update:} $\theta_{t+1} \leftarrow \theta_t - \alpha_t\delta\nabla_{\theta_t}Q(x,a;\theta_t)$.\\
\textbf{Experience Replay:} Collect $D=\{(x,a,r,x')\}$, fix $\theta_\mathrm{old}$. Minimize $L(\theta) = \sum_{d\in D}l_2(d,\theta)$.
\textbf{DDQN:} Reduce \textbf{maximization bias} by changing $\max_{a'}Q(x',a';\theta_\mathrm{old}))$ in $\delta$ to $\max_{a'}Q(x',a';\theta))$. 

\subsection{Model-free, $|A|$ large, $|X|$ large}
\textbf{Policy gradient:} Assume episodic setting. Define policy $\pi_\theta(a|x)$. Define \emph{roll-out} $\tau_{0:t}=(x_0,a_0,r_0,\ldots,x_t,a_t,r_t,x_{t+1})$\\
\emph{Goal:} Maximize $J(\theta) = \E_{\tau\sim\pi_\theta}[r(\tau)]$\\
\emph{Training:} GD on $\nabla_\theta J(\theta) = \nabla_\theta \E_{\tau\sim\pi_\theta}[r(\tau)] = $\\$\E[r(\tau)\nabla_\theta\log\pi_\theta(\tau)] = \E[r(\tau)\sum_{t=0}^T\nabla\log\pi_\theta(a_t|x_t)]$.
\textbf{Baselines\textnormal{\emph{(for variance reduction):}}}
For baseline $b(\tau_{0:t-1})$ we have $\E[r(\tau)\sum_{t=0}^T\nabla\log\pi_\theta(a_t|x_t)] = \E[\sum_{t=0}^T(r(\tau)-b(\tau_{0:t-1}))\nabla_\theta\log\pi_\theta(a_t|x_t)]$.\\
\textbf{REINFORCE}: Baseline $b(\tau_{0:t-1}) = \sum_{t'=0}^{t-1} \gamma^{t'}r_t'$ so that $J(\theta) = \E[\sum_{t=0}^T\gamma^t G_t\nabla\log\pi_\theta(a_t|x_t)]$ where $G_t = \sum_{t'=t}^T\gamma^{t'-t}r_{t'}$.

\subsection{Actor-critic methods}
\textbf{Policy gradient theorem:} 
$\nabla J(\theta_\pi) = \E_{\tau\sim\pi_{\theta_\pi}}[r(\tau)] = \E_{(x,a)\sim\pi_\theta}[Q^{\pi_{\theta_\pi}}(x,a)\nabla\log\pi_{\theta_\pi}(a|x)]$.\\
Define \textbf{actor} $\pi_\theta(a|x)$, \textbf{critic} $Q_{\theta_Q}(x,a)$. \emph{Training:} $\theta_\pi \mathrel{{+}{=}} \eta_t Q_{\theta_Q}(x,a)\nabla\log\pi_{\theta_\pi}(a|x)$ (by PGT).\\
$\theta_Q \mathrel{{-}{=}} \eta_t(Q_{\theta_Q}(x,a)-r-\gamma Q_{\theta_Q}(x',\pi_{\theta_\pi}(x')))\nabla Q_{\theta_Q}(x,a)$
(sim. to DQN, critic update non-episodic.)
\textbf{A2C:} Add baseline $-V_{\theta_V}(x)$ to get
$\theta_\pi \mathrel{{+}{=}} \eta_t A_{\theta_A}(x,a)\nabla\log\pi_{\theta_\pi}(a|x)$ where $A_{\theta_A}(x,a) = Q_{\theta_Q}(x,a) - V_{\theta_V}(x)$ is the \textbf{advantage function} ($A^\pi(x,a) = Q^{\pi}(x,a)-\E_{a'\sim\pi(x)}(Q^{\pi}(x,a'))$).\\
\textbf{TRPO\&PPO}: For $\mathrm{KL}(\theta||\theta_k)\leq\delta$, find $\theta_{k+1} = \argmax_\theta \E_{(x,a)\sim\pi_{\theta_k}}[\frac{\pi_\theta(a|x)}{\pi_{\theta_k}(a|x)}A^{\pi_{\theta_k}}(x,a)]$ (mildly off-policy).\\
\textbf{DDPG:} Estimate and replace $\max_{a'}Q_{\theta_\mathrm{old}}(x',a')$ with $Q_{\theta_{\mathrm{old}}}(x', \pi_{\theta_\pi}(x'))$ in Q-learning. Then, $\theta^*_{\pi}\in\argmax_\theta \E_{x\sim\mu}[Q_{\theta_Q}(x,\pi_\theta(x))]$ for any $\mu > 0$. \emph{Algorithm:} 1. Observe $\{(x,a,x',r)\}$ with actions $a=\pi_{\theta_\pi}(x)+\epsilon$, $\epsilon\sim\mathcal{N}(0,\lambda\mathbf{I})$ (\emph{noise for exploration}). 2. For minibatches $B\subset D$:\\
$\theta_Q \leftarrow \mathrm{DQN}$\\
$\theta_\pi \leftarrow \theta_\pi +\eta\nabla\frac{1}{|B|}\sum_B Q_{\theta_Q}(x,\pi_{\theta_\pi}(x))$\\
$\theta^{\mathrm{old}}_{\pi | Q} \leftarrow (1-p)\theta^{\mathrm{old}}_{\pi | Q} + p\theta_{\pi | Q}$.\\
\textbf{TD3:} DDPG with another critic for updating $\theta_\pi$ --- combats maximization bias.\\
\textbf{SVG:} DDPG with \textbf{randomized policies} $\pi_{\theta_\pi}(a|x)$. Update $\theta_Q$ by sampling $a'\sim\pi_{\theta^\mathrm{old}_\pi}(x')$ to estimate $\E_{a'\sim\pi}[r+\gamma Q_{\theta^\mathrm{old}_Q}(x',\pi_{\theta^\mathrm{old}_\pi}(x'))]$. Update $\theta_\pi$ by reparameterizing $a\sim\pi_{\theta_\pi}$ to $a=\psi_{\theta_\pi}(x,\epsilon)$.
Then, $\nabla_{\theta_\pi}\E_{a\sim\pi_{\theta_\pi}(x)}Q_{\theta_Q}(x,a) = \E_\epsilon[\nabla_{\delta_\pi}Q_{\theta_Q}(x,\psi_{\theta_\pi}(x,\epsilon))]$.\\
\textbf{SAC:} SVG with entropy regularization: $J_\lambda(\theta) = J(\theta) + \lambda H(\pi_\theta)$ (helps exploration).

\subsection*{Model-based, $|A|$ large, $|X|$ large}
\emph{Algorithm:} 1. Collect data $D$ with policy $\pi$.\\
2. Est. trans. model $f(x,a)$, rew. $r(x,a)$ from $D$.\\
3. Plan $\pi$ from $f$, $r$, $D$.{\tiny\CircArrowRight{}}
\subsubsection*{3. --- Planning with Model Predictive Control}
For horizon $H$, define $J(a_{t:t+H-1})=\sum_{\tau=t:t+H-1}\gamma^{\tau-t}r_\tau(x(a_{t:\tau-1}), a_\tau)$.\\
1. Find $a_t,\ldots = \argmax_{a_{t:t+H-1}}J(a_{t:t+H-1})$ where
$x_{t+1} = f(x_t, a_t)$.\quad | \quad 2. Execute $a_t$.{\tiny\CircArrowRight{}}\\
\emph{Optimization:} GD, random shooting (sample $(a_{t:{t+H-1}})^{(i)}$, choose $\color{black}\argmax$).\\
\textbf{MPC + Value Estimate:} Find $a_t,\ldots = \argmax_{a_{t:t+H-1}}J(a_{t:t+H-1})+\gamma^H V(x(a_{t:t+H-1}))$.\\
\textbf{MPC + Probabilistic transition model:}
Redef. $J(a_{t:t+H-1})=\E_{x_{t+1}:x_{t+H}}[$\\$\sum_{\tau=t:t+H-1}\gamma^{\tau-t}r_\tau(x_t, a_\tau)|a_{t:t+H-1}]$ = $\E_{\epsilon_{t}:\epsilon_{t+H-1}}[$\\$\sum_{\tau=t:t+H-1}\gamma^{\tau-t}r_\tau(x_\tau(a_{t:\tau-1},\epsilon^{(i)}_{t:\tau-1}), a_\tau)]$ for reparam. trans. model $x_t+1 = f(x_t,a_t,\epsilon_t)$.\\
\textbf{Parameterized policies:}
Def. policy $\pi_\theta(x)$.\\
\hphantom\,\,\,\,\,Like DDPG, max. $\E_{x\sim\mu}[Q^{\pi_\theta}(x,\pi_\theta(x))] = \E_{x\sim\mu}[\sum_{\tau=0:H-1}\gamma^\tau r_\tau + \gamma^H Q(x_{H}, \pi_\theta(x_H))]$.
\emph{Replaces slow MPC planning with fast policy evaluation.}

\subsubsection*{2. --- Learning $f$ and $r$}
Supervised learning problem with inputs $(x_i, a_i)$ and labels
$(x_{i+1}, r_i)$.\\
\textbf{MAP estimate of $p(f|D)$:} Define prior $p(\theta)$.\\ Def. lik. as \textbf{conditional Gaussian dynamics}: $x'|f, x, a \sim \mathcal{N}(\mu_f(x,a), \Sigma_f(x,a))$.
\emph{Problem: Creates large compound error for large horizon H $\color{black}\implies$ poor performance. Solution: Bayesian models.}
\textbf{Bayesian models (GPs, BNNs):} Sample $f^{(i)}\sim p(\cdot|D)$, approximately maximize $\E_{f|D}[J(a_{t:t+H-1})]\approx \hat{J}(a_{t:t+H-1}) = \frac{1}{M}\sum_{i\leq M}(\sum_{\tau=t:t+H-1}\gamma^{\tau-t}r_\tau(x_\tau(a_{t:\tau-1},\epsilon^{(i)}_{t:\tau-1}, f^{(i)}), a_\tau)+\gamma^H V(x_{H}))$. \textbf{PETS} does this with an NN ensemble $f^{(i)}$.
\textbf{Exploration-exploitation dilemma:} Act so that we get good samples for supervised learning. \emph{Sol. 1: Additive noise.}\\
\emph{Sol. 2:} \textbf{Thompson sampling:} Sample $f\sim P(\cdot|D)$, plan $\pi=\argmax_{\pi}J(\pi,f)$, roll-out $\pi$.\\
\emph{Sol. 3:} \textbf{Optimistic exploration:} Define set of plausible models given data $D$: $M(D)$. Plan $\pi = \argmax_\pi \max_{f\in M(D)}J(\pi, f)$ (\emph{difficult}).\\
\textbf{H-UCRL:} Optimistic exploration with $\pi_{t+1} = \argmax_{\pi}\max_{\eta(\cdot)\in[-1,1]^p} J(\tilde{f},\pi)$ and $\tilde{f}(x,a) = \mu_t(x,a) + \beta_{t}\Sigma_{t}(x,a)\eta(x,a)$.\\
\textbf{Safe exploration:} \emph{Th.:} With conditional Gaussian dynamics, can overapprox. reachable states w.p. $1-\delta$.