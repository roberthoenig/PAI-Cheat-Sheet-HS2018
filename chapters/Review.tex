\section{Review of useful concepts and Introduction}
\subsection{Multivariate Gaussian}
%$\sigma =$ covariance matrix, $\mu$ = mean\\
$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$

Suppose we have a Gaussian random vector $X_V \sim N(\mu_V, \Sigma_{VV})$.\\
Suppose we take two disjoint subsets of V: $A={i_1,...,i_k}$ and $B={j_1,...,j_m}$.\\
Then, the conditional distribution: \\
$P(X_A|X_B=x_B)=N(\mu_{A|B}, \Sigma_{A|B})$ is Gaussian:\\
$\mu_{A|B}=\mu_A+\Sigma_{AB}\Sigma^{-1}_{BB}(x_B-\mu_B)$\\
$\Sigma_{A|B}=\Sigma_{AA}-\Sigma_{AB}\Sigma^{-1}_{BB}\Sigma_{BA}$

\subsection{Information Theory}
$I(Y;X) = I(X;Y) = H(X) - H(X|Y)$.\\
$I(X;Y|Z) = H(X|Z) - H(X|Y;Z)$.\\
$H(X|Y) = \E_{y\sim p(y)}[X|Y=y]$.\\
If $X\sim\mathcal{N}(\mu, \Sigma)$ then $H(X) = \frac{1}{2}\ln(2\pi e)^d|\Sigma|$.\\
If $X\sim\mathcal{N}(\mu, \Sigma)$, $Y = X + \epsilon$, $\epsilon\sim\mathcal{N}(0,\sigma^2\mI)$, then $I(X;Y) = \frac{1}{2}\ln|\mI+\sigma^{-2}\Sigma|$.

\section{Formulas}
\textbf{Hoeffding's inequality:} I.i.d. $(X_i)\sim X\in[0,C]$ satisfy $P(|\E[X] - \frac{1}{n}\sum_{i\leq n}X_i|>\epsilon) \leq 2\exp(-2n\frac{\epsilon^2}{C^2})$.

\section{Bayesian Learning}
\textbf{Prior:} $p(\theta)$
\textbf{Likelihood:} $p(\rvy|\mX,\theta) = \prod p(y_i|x_i,\theta)$
\textbf{Posterior:} $p(\theta|\rvy,\mX) = \frac{1}{Z}p(\theta)\prod p(y_i|x_i, \theta)$ with $Z = \int p(\theta)\prod p(y_i|x_i, \theta) d\theta$.
\textbf{Predictions:} $p(y^*|x^*,\mX,\rvy) = \int p(y^*|x^*,\theta)p(\theta|\mX, \rvy) d\theta$. 


\section{Bayesian Neural Networks}
\textbf{Prior:} $p(\theta) = \mathcal{N}(0, \sigma^2\mI)$.\\
\textbf{Likelihood:} $p(y|\vx, \theta) = \mathcal{N}(\mu_\theta(\vx), \sigma^2_\theta(\vx))$. ($\sigma^2_\theta(\vx)$ non-constant $\color{black}$ iff het.scd. noise).
\textbf{Predictions:} \mbox{$p(y^*|\mX,\vy,\vx^*) \approx \frac{1}{m}\sum\mathcal{N}(y^*;\mu_{\theta_m}(\vx^*), \sigma_{\theta_m}^2(\vx^*))$ (GMM).} \mbox{$\Var(y^*|\mX,\rvy,x^*) = {\color{red}\E[\Var(y^*|x^*,\theta)]} + {\color{blue}\Var(\E[y^*|x^*,\theta])}$} $\approx {\color{red}\overset{aleatoric}{\frac{1}{m}\sum\sigma^2_{\theta_m}(\vx^*)}} + {\color{blue}\overset{epistemic}{\frac{1}{m}\sum(\mu_{\theta_m}(\vx^*)-\bar{\mu}_{\theta_m}(\vx^*))^2}}$\\
\textbf{Variational Inference:} Approx. $p(\theta|\rvy) \approx q_\lambda(\theta)$. Minimize $\mathcal{L}(\lambda) = \mathrm{KL}(q_\lambda||p(\cdot|\rvy)) = \E_{\theta\sim q_\lambda}[\log p(\rvy|\theta)] - \mathrm{KL}(q_\lambda||p(\cdot)) = $ \textbf{ELBO}. \textbf{Bayes-by-backprop:} Min. $\mathcal{L}(\lambda)$ with SGD + reparam.\\
\textbf{MCMC:} Use MALA / SGLD to sample $\theta^{(t)} \sim p(\theta|\rvy)$. \emph{Problem:} Storing many $\theta^{(t)}$ for multiple preds. expensive. \emph{Solution:} Approximate $\theta^{t}_i \sim \mathcal{N}(\textrm{sample mean, var})$.\\
\textbf{Dropout as BNN:} Use posterior $q_\lambda(\theta) = \prod q_{\lambda_i}(\theta_i)$ with $q_{\lambda_i}(\theta_i) = p\delta_0(\theta_i) + (1-p)\delta_{\lambda_i}(\theta_i)$.
\textbf{Probabilistic ensembles:} Compute $\theta^{(t)}$ using MAP with bootstrapping.
\textbf{Calibration:} Perfectly calibrated iff $\forall p.\,P(\hat{Y}=Y|\hat{P}=p) = p$, where $Y$ is the real class and $\hat{Y}, \hat{P}$ is the NN output. \textbf{Expected Calibration Error:} $\E[|P(\hat{Y}=Y|\hat{P}) - \hat{P}|]$.
\textbf{Max. Calibration Error:} $\max_p |P(\hat{Y}=Y|\hat{P}=p) - p|$.

\section{Bayesian Linear Regression}
\textbf{Prior:} $\rvw\sim\mathcal{N}(\mathbf{0},\Lambda^{-1})$\\
\textbf{Posterior:} $p(\rvw;\mathbf{X},\mathbf{y}) = \mathcal{N}(\rvw; \bar\mu, \bar\Sigma)$ where\\
$\bar\mu = (\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}\mathbf{X}^T\mathbf{y}$\\
$\bar\Sigma = \sigma^2(\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}$\\
\textbf{Pred.:}
\mbox{$y^* = {\vx^*}^T\rvw + \epsilon\sim \mathcal{N}({\vx^*}^T\bar\mu, \underbrace{{\vx^*}^T\bar\Sigma{\vx^*}}_\text{\color{purple}epistemic}+\underbrace{\sigma^2}_\text{\color{purple}aleatoric})$}
\textbf{\color{gray}Pred:}
$p(y^*|\rvx^*,\mX,\rvy) = \int p(y^*|\rvx^*,\rvw)p(\rvw|\mX,\rvy) d\rvw$

\section{Kalman Filters}
\textbf{Transition model}:
\mbox{$P(\rvx_{t+1}|\rvx_t)=\mathcal{N}(\rvx_{t+1};F\rvx_t, \Sigma_x)$}\\
\textbf{Sensor model}:
$P(\rvy_{t}|\rvx_t)=\mathcal{N}(\rvy_{t};H\rvx_t, \Sigma_y)$\\
\textbf{Conditioning:} $P(\rvx_t|\rvy_{1\ldots t}) = \mathcal{N}(\rvx_t;\mu_t, \Sigma_t)$\\
\textbf{Prediction:} $P(\rvx_{t+1}|\rvy_{1\ldots t}) = \mathcal{N}(\rvx_{t+1};F\mu_t, \Sigma_t + \Sigma_x)$\\
\textbf{Kalman gain}:
$K_{t+1}=$\\$(F\Sigma_t F^T+\Sigma_x)H^T(H(F\Sigma_t F^T+\Sigma_x)H^T+ \Sigma_y)^{-1}$\\
\textbf{Kalman update}:\\
$\mu_{t+1}=F\mu_t+K_{t+1}(\rvy_{t+1}-HF\mu_t)$\\
$\Sigma_{t+1}=(I-K_{t+1}H)(F\Sigma_tF^T + \Sigma_x)$\\
% asdf